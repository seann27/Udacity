Population Segmentation
- train and deploy unsupervised models to group US counties
More about SageMaker:
    - end to end machine learning service
        -> building, training, tuning, and deploying models
    - applications (products at scale)
        -> used to process large amounts of data
        -> sports analytics, streaming data allows real-time insight (next gen stats)
    - skills with using SageMaker
        -> users gain experience with data collection, picking the right models,
           tuning hyperparameters, and updating models
        -> once the user understands the process, can use SageMaker's API to
           automate the workflow
    - can use ground truth concept to learn from human annotations
        -> saves 70% of time that it would take to manually annote labels

K-Means clustering (unsupervised learning method)
-> group data points that have similar traits
-> case: used for clustering images
-> can give it unlabeled data of n dimensions

Data exploration is very important
    - notice any initial patterns in data distribution and features
    - normalize the features with either one-hot encoding or mix-miax/log scaling
        -> converts all cells in dataframe to numerics for machine learning algorithms
    - PCA
        -> principal components are weighted combinations of existing features
            - features that are uncorrelated
            - account for largest variability
        -> scikit learn's PCA class automatically centers data
    - good general rule -> start with n-1 components when first defining pca model

- Data must be formatted as a RecordSet in order to be used with SageMaker's models
- convert df to np float array
    -> np_data = df.values.astype('float32')
    -> formatted_data = sagemaker_model.record_set(np_data)
- train model
    -> model.fit(formatted_data)
- get explained variance for components
- Exploring the makeup of each component (features that define it)
    -> generate dataframe of weights and features
    -> take abs value of weight, sort dataframe by weights
    -> plot to view most heavily weighted features for a specific component
- When the data is clustered:
    -> view all rows for a specific cluster
    -> determine what principal component explains the most variance in the cluster
    -> examine the components features for a better understanding

Overview up to this point:
1) preprocess data into numerical, scaled values (training data)
2) train a PCA model (with training data) with components = number features - 1
3) determine how many components (in this case 5) are needed for an explained variance above a
   certain threshold (in this case 80%)
4) deploy this model
5) use model to predict training data
6) grab top n components (determined from step 3) from the predicted PCA
7) create new dataframe with only these components as features
8) run new dataframe through deployed kmeans clustering algo
   - optimize # of clusters by determining where the cluster center distance "elbows"
   - want points to be somewhat evenly distributed between clusters (even cluster counts)
9) use heat map to determine how heavily the components influence the clusters
   - look at a cluster, determine what component(s) influence it, examine component for
     what features define it
   - filter indexes for what original data points are mapped to the particular cluster
