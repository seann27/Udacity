Supervised Learning:
1) Intro:
    a) Combination of statistics and computer science
    b) Instead of endless tree of if-else statements, machine learning employs computers to recognize patterns in data
    c) Avoid hardcoding rules using math and statistics
    d) Supervised learning
        i) classification
            - categorize items
            - predict groups
        ii) regression
            - predict numerical value
    e) deep learning barriers
        i) must have enough data
        ii) computing power
        iii) won't understand decisions being made due to complexity
    f) Scikit learn - provides supervised/unsupervised machine learning modules for data scientists
    g) Human biases in data are reflected in ML models

Linear Regression:
    a) fitting a line through set of points
        i) y=mx+b
        ii) adjusting the line for coordinates (p,q), y=(m+pr)x+(b+r)
            a) multiplying by "p" takes into account negative numbers and adjusts the slope correctly
        - absolute trick: y=(m+pr)x+(b+r)
            - NOTE: operator changes if point is below line (subtract everything)
        - square trick: y=(m+p(y2-y1)r)+(b+(y2-y1)r)
            - operator is always addition
    b) minimize error (deviation) of line to points
        i) use gradient descent to make small movements that get points closer to line
    c) Mean absolute error and mean squared error are error functions for linear regression
        i) mean error -> average of the |y2-y1| values
        ii) mean sq error -> average of the (y2-y1)^2 values divided by 2
    d) two algorithms for fitting a line -> the tricks (a) and the error functions (c)
***
- # scikit learn's linear regression class provides a fit() function
- from sklearn.linear_model import LinearRegression
- model = LinearRegression()
- model.fit(x_values,y_values)
- model has a predict() function that predicts labels given new data
