Introduction:
- Understanding grouping with unlabeled data
    - figure out what regions of data share similarity with one another
- Used for grouping data (can narrow down or discover important features)

Topics:
1) Clustering
2) Hierarchical and Density Based Clustering
3) Gaussian Mixture Models and Cluster Validation
4) Principal Component Analysis
5) Random Projection and Independent Component Analysis

Clustering:
- K-means algorithm
    - how many clusters you want the data to separate into
    - how to choose k?
        - if you know categories, can use that as estimate
        - otherwise can use a decision method
    - elbow method
        - increasing k until the impact is substantially low (avg distance between clusters doesn't change much)
        - use a scree plot to determine the exact falloff
    - Steps:
        1) centroids placed randomly
        2) points are assigned a centroid based on proximity
        3) centroids are moved to the center of the points assigned
    - The first step matters and can yield different results
    - The algorithm is performed many times with different random placement of 1st step centroids
    - The final clustering that yields the smallest avg. distance wins
- Hierarchical clusterings
    - all techniques are a part of agglomerative clustering in sklearn
    - starts from the bottom up, assuming each point is a cluster
        - first iteration, merges nearest 2 clusters
    - measures distance by farthest possible link between clusters
    - can also do it by average between all points between a set of clusters
    - ward's method -> find central point between clusters and square the sum of distances between all points
      in each cluster to the central point
      - subtract the variance (distance from points to central point of an individual cluster)
    - in sklearn, import cluster
        - use agglomerative clustering
        - specify number of clusters and linkage method
    - use scipy for drawing dendrograms
    - hierarchical representations are very informative
        - potent when data reflects hierarchical relationships (evolution in biology)
        - con -> sensitive to noise and outliers
        - con -> computationally intense
    - DBSCAN (Density Based Spatial Clustering of Applications with Noise)
        - groups together points that are compactly grouped together
        - good at detecting outliers and noise
- Gaussian Mixture Model Clustering
    - Every point belongs to every cluster with different levels of membership
    - uses probabilities and statistics to come up with distributions of point memberships between clusters
    - based off of normal distributions (uniformly distributed histograms for example)
        - bell curves
        - mean/average
        - standard deviation
        - mean + or- (i*std) used to scale the distribution where i=1,2,3...n
    - multivariate gaussian distribution used to handle multiple dimensions and variables
    - pros -> soft-clustering (sample membership of multiple clusters)
    - pros -> cluster shape flexibility
    - cons -> sensitive to init values
    - cons -> local optimum
    - cons -> slow convergence rate
- Gaussian implementation:
    1) initialize k gaussian distributions
        - give each k distribution a mean and std
        - better way to do it is to run kmeans to get these metrics
    2) soft cluster data - "Expectation step"
        - calculate the membership of each point to each cluster based on the features
        - use formula (probability density function of a normal distribution)
    3) re-estimate the gaussians - "maximization step"
        - calculate the weighted average of all the points in a particular cluster
            - uses features and membership percentage
        - calculate weighted variance
    4) evaluate the log-likelihood to check for convergence
        - the higher this value, the more confident we are that the model is correct
        - can tweak parameters for better optimization (in the initialization step)
        - change the covariance type
    5) repeat from step 2 until converged
- Cluster Analysis:
    1) feature selection/extraction
    2) choose a clustering algorithm/selection tuning
        - proximity measure (calculate distance between points)
    3) cluster validation
        - number of scoring methods to evaluate the quality of the structure
    4) results interpretation
        - what insights we can learn from the clusters
- Cluster validation
    - cluster validation indices
        - external indices (predetermined labels)
        - internal indices (measure fit between data and structure using only the data)
        - relative indices
    - adjusted rand index (scoring example) for external indices
        - between -1 and 1
        - compares using labels (grand truth) to how the data was cluster
    - internal indices scoring example -> silhouette coefficient
        - between -1 and 1
        - there is a silh. coeff. for every point in the dataset that can be calculated
            - compares distances of points in same clusters, other neighboring clusters. and other clusters
            - average all of them for the final coefficient for the result
            - used to find what k is
        - can also use to compare how different algorithms performed on same dataset
            - doesn't measure well with dbscan
        - not always good at measuring
        - always looking for compact and dense clusters (poor performance for the 'rings' dataset)
        - should use something like dbsv (density based cluster validation) for DBSCAN algorithm

Principal Component Analysis:
- Transforming data rather than clustering it
- Allows you to retain informative parts of the data but with less of it
- Used for condensing the dataset
- Feature selection and feature extraction are useful for filtering out unimportant features
- Interpreting results:
    - principal components
    - variability of the original data
- reduce features into groupings called latent features
- Feature Selection:
    - finding a subset of the original features that are determined to be most important
    - filter methods
    - wrapper methods
- Feature Extraction:
    - construct new features called "latent features"
    - existing features are combined as "principal components"
    - want components that capture the largest amount of variance in the dataset
    - components must be orthogonal to each other
- when to use PCA?
    - to reduce dimensionality of the data
    - find latent features that might encapsulate other features

Random Projection:
- computationally less expensive than PCA
- multiplies dataset by a random matrix
- Johnson-Lindenstrauss lemma
    - dataset can be mapped down to much smaller dimensionality in a way where
      distances between points are minimally effected
- how are they preserved?
    - (1-eps)*(u-v)^2 < projections < (1+eps)*(u-v)^2
    - use epsilon as a hyperparameter to specify range of error in projections

ICA (Independent Component Analysis):
- isolate features that share different sources (Independent of one another)
    - blind source separation
- An audio track with 3 different instructments playing different songs
    - pick out individual instruments/songs
- Take an input matrix, multiple by an estimated unmixing matrix, see if output
  matrices converge, repeat until complete source separation occurs
