Steps for creating and testing a two-layer NN model:
1) Load data
2) One-hot encode categorical features
3) scale the numerical data (fits between 0-1)
4) split data into training and testing sets
5) define the following functions:
    a) sigmoid function
        i) 1/(1+e^-x) -> 1/(1+np.exp(-x))
    b) sigmoid prime function (derivative)
        i) sigmoid(x) * (1-sigmoid(x))
    c) error formula
        i) -y * np.log(output) - (1 - y) * np.log(1-output)
    d) error term
        i) (y-output) * sigmoid_prime(x)
    e) train_function
        i) loop through epochs
            - initialize weight changes (del_w = np.zeros(weights.shape))
            - loop through records
                i) output = sigmoid(np.dot(x,weights))
                ii) error = error_formula(y,output)
                iii) error_term = error_term_formula(x,y,output)
                iv) del_w += error_term * x
            - apply changes to weights
                i) weights += learnrate * del_w / n_records
        ii) return weights
    f) weights = train_function(features, targets, epochs, learnrate)
    g) calculate the accuracy:
        i) output = sigmoid(np.dot(features_test,weights))
        ii) predictions = test_output > 0.5 # this means activated nodes
        iii) accuracy = np.mean(predictions == targets_test)

Implementing Gradient Descent:
1) define sigmoid and sigmoid prime functions
2) get output -> sigmoid(np.dot(x,weights))
3) error -> (y-output)
4) gradient -> sigmoid_prime(output)
5) error term -> error * gradient
6) apply to weights -> del_w = learnrate * error_term * x

*** Multilayer NN ***
Params:
- X         # raw input for first layer
- weights_1 # weights for first layer
- weights_2 # weights for second layer

Forward Pass:
1) calculate input to hidden layer
    i) input_1 = X * weights_1
2) calculate hidden layer output
    i) output_1 = sigmoid(input_1)
3) calculate input to output layer
    i) input_2 = output_1 * weights_2
4) calculate output of the network
    i) output_final = sigmoid(input_2)

Backpropagation:
- The formula for calculating the change in weights for a layer is product of:
    - learning rate
    - error_term for that layer's output
        - weights of prev layer * prev layer's error term * sigmoid_prime(prev layer's raw input)
        - if no prev layer (current layer is final output aka first step):
            - total error * sigmoid_prime(final output)
    - layer's input
# using the values from the forward pass algo above:
1) calculate error term of final output
    i) error = y-output_final
    ii) output_error_term = error * sigmoid_prime(output_final)
2) calculate error term of hidden layer
    i) hidden_error_term = weights_hidden_to_output * output_error_term * sigmoid_prime(output_1)
3) calculate del_w for output layer
    i) del_w_o = output_error_term * output_1
4) calculate del_w for hidden layer
    i) del_w_h = hidden_error_term * X
5) for every epoch, update weights:
    i) weights_hidden_layer += del_w_h * learnrate / n_records
    ii) weights_output_layer += del_w_o * learnrate / n_records

* NOTE - Whenever there is a choice between simple model and a slightly better but much more complicated model, typically defer to simple model
* NOTE - NN architecture is very difficult to get right, almost always end up under/overfitting
    - Approach -> lean towards overfitting, optimize from there
* NOTE - Use a model complexity graph to plot training vs testing results
    - Approach -> use model with highest convergence and lowest error
* NOTE - Punish large weights (coefficients) to reduce overfitting
    - sum of abs or sum of squares (L1 and L2 regularization respectively)
    - lambda * (sum weights)
    - L1 is good for feature selection
    - L2 is good for training models
* NOTE - Concept of dropout, inactivate random nodes to improve training of the rest of them
* NOTE - use the concept of momentum to overcome problems with local minima
    - Takes a constant between 0 and 1, raises it to the power of how far back a step is
        - previous step is B, the next previous is B^2, etc...
    - takes average of all these values and applies it to the current step

Creating a neural network with PyTorch:
1) imports needed:
    import torch (base)
    from torch import nn (modeling)
    from torch import optim (training the model)
    import torch.nn.functional as F
2a) build model using nn.Sequential()
    a) use ReLU() activation functions for  hidden layers
    b) use LogSoftmax for final activation
        i) this is used in conjunction with a loss function (criterion)
        ii) used when need to calculate multiple class probabilities
            - e.g. 10 different output image labels
2b) build model by creating your own class
    a) class Classifier(nn.Module):
        def __init__(self):
            super().__init__()
            self.hidden1 = nn.Linear(784,256)
            self.hidden2 = nn.Linear(256,128)
            self.hidden3 = nn.Linear(128,64)
            self.output = nn.Linear(64,10)
        def forward(self, x):
            # flatten data
            x = x.view(x.shape[0],-1)

            # run pipeline
            x = F.relu(self.hidden1(x))
            x = F.relu(self.hidden2(x))
            x = F.relu(self.hidden3(x))
            x = F.log_softmax(self.output(x), dim=1)

            return x

3) define optimizer and loss functions (example):
    a) optimizer = optim.SGD(model.parameters(), lr=0.005)
    b) criterion = nn.NLLLoss()
4) train the model
    a) loop over epochs
    b) loop over inputs/labels
        i) convert to 2D tensor (make sure same size as NN input layer)
        ii) clear optimizer
            - optimizer.zero_grad()
        iii) forward pass
            - model.forward(input)
        iv) calculate loss
            - criterion(output,labels)
        v) perform Backpropagation
            - loss.backward()
        vi) adjust the weights
            - optimizer.step()

Implementing dropout to reduce overfitting
1) by modifying the existing model class:
    a) class Classifier(nn.Module):
        def __init__(self):
            super().__init__()
            self.hidden1 = nn.Linear(784,256)
            self.hidden2 = nn.Linear(256,128)
            self.hidden3 = nn.Linear(128,64)
            self.output = nn.Linear(64,10)

            self.dropout = nn.Dropout(p=0.2)

        def forward(self, x):
            # flatten data
            x = x.view(x.shape[0],-1)

            # run pipeline w/ dropout
            x = self.dropout(F.relu(self.hidden1(x)))
            x = self.dropout(F.relu(self.hidden2(x)))
            x = self.dropout(F.relu(self.hidden3(x)))

            # output -> no dropout here
            x = F.log_softmax(self.output(x), dim=1)

            return x
2) can toggle dropout off/on with model.eval() and model.train() respectively

Saving/Loading models (fc.model is a custom module created by instructor):
1) Save:
    a) save the model architecture along with the state_dict in variable
        - checkpoint = {  'input': 784,
                          'output': 10,
                          'hidden_layers': [each.out_features for each in model.hidden_layers],
                          'state_dict': model.state_dict()}
                       }
    b) torch.save(checkpoint, 'checkpoint.pth')
2) Load:
    a) As a function (will need to be tailored/changed for every model built):
        def load_checkpoint(filepath):
            checkpoint = torch.load('checkpoint.pth')
            model = fc.model.Network(checkpoint['input_size'],
                                     checkpoint['output_size'],
                                     checkpoint['hidden_layers'])
            model.load_state_dict(checkpoint['state_dict'])

            return model

Loading real world image data
1) imports needed:
    - import torch
    - from torchvision import datasets, transform
2) Load dataset from image folder
    a) dataset = datasets.ImageFolder('path/to/data', transform=transform)
    b) expects the following formatted files:
        root/dog/xzy.png
        root/cat/fdsfd.png
3) transform your data
    a) use a compose pipeline with different transforms
        i) transform = transforms.Compose([ transforms.Resize(255),
                                            transforms.CenterCrop(224),
                                            transforms.ToTensor() ])

Transfer Learning -> using pre-trained models to classify images
https://pytorch.org/docs/0.3.0/torchvision/models.html
1) make sure your dataset of images has the correct configuration for the models
    a) transform with 224x224 and normalize with the means/std
    b) transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
2) choose a model
    a) model = models.densenet121(pretrained=True)
3) freeze the parameters so the model's features aren't retrained and set classifier
    a) for param in model.parameters();
        param.requires_grad = False
    b) from collection import OrderedDict
    c) classifier = nn.Sequential(OrderedDict([
                              ('fc1', nn.Linear(1024, 500)),
                              ('relu', nn.ReLU()),
                              ('fc2', nn.Linear(500, 2)),
                              ('output', nn.LogSoftmax(dim=1))
                          ]))
    d) model.classifier = classifier

Graphing results:
# change this to the trainloader or testloader
data_iter = iter(testloader)

images, labels = next(data_iter)
fig, axes = plt.subplots(figsize=(10,4), ncols=4)
for ii in range(4):
    ax = axes[ii]
    helper.imshow(images[ii], ax=ax, normalize=False)

image, label = next(iter(trainloader))
